{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../dataset/')\n",
    "sys.path.append('../network/')\n",
    "sys.path.append('../model/')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import joblib\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from main_loading import *\n",
    "from main_network import *\n",
    "from main_model_rec import *\n",
    "from main_model_one_class import *\n",
    "from scipy.spatial import KDTree\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "root = '/net/leksai/data/FashionMNIST'\n",
    "rec_model_path = '/net/leksai/nips/model/rec/fmnist/rec_unsupervised_[2]_[]_[0.0]/net_fmnist_LeNet_rec_eta_100_epochs_150_batch_128/model.tar'\n",
    "oc_model_path = '/net/leksai/nips/model/one_class/fmnist/one_class_unsupervised_[2]_[]_[1]_[0.0]/net_fmnist_LeNet_one_class_eta_100_epochs_150_batch_128/model.tar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Only the Encoder Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## For One-Class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneClassEncoder:\n",
    "    def __init__(self):\n",
    "        self.net = None\n",
    "        self.net_name = None\n",
    "\n",
    "    def set_network(self, net_name):\n",
    "        self.net_name = net_name\n",
    "        self.net = build_network(net_name)\n",
    "\n",
    "    def load_model(self, model_path, map_location):\n",
    "        model_dict = torch.load(model_path, map_location=map_location)\n",
    "        self.c = model_dict['c']\n",
    "        self.net.load_state_dict(model_dict['net_dict'])\n",
    "\n",
    "    def test(self, train, dataset, device, batch_size, n_jobs_dataloader):\n",
    "        if train:\n",
    "            all_loader, _ = dataset.loaders(batch_size=batch_size,\n",
    "                                            num_workers=n_jobs_dataloader)\n",
    "        else:\n",
    "            all_loader = dataset.loaders(batch_size=batch_size,\n",
    "                                         num_workers=n_jobs_dataloader)\n",
    "        net = self.net.to(device)\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        n_batches = 0\n",
    "        X_pred_list = []\n",
    "        net.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in all_loader:\n",
    "                X, y, idx = data\n",
    "                X, y, idx = X.to(device), y.to(device), idx.to(device)\n",
    "\n",
    "                X_pred = net(X)\n",
    "                X_pred_list += X_pred\n",
    "        \n",
    "        return np.array(X_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "oc_encoder = OneClassEncoder()\n",
    "oc_encoder.set_network('fmnist_LeNet_one_class')\n",
    "oc_encoder.load_model(oc_model_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## For Reconstruction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecEncoder:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.net_name = None\n",
    "        self.net = None\n",
    "        self.ae_net = None\n",
    "\n",
    "\n",
    "    def set_network(self, net_name: str='fmnist_LeNet_one_class'):\n",
    "        \"\"\"\n",
    "        Set the network structure for the model.\n",
    "        The key here is to initialize <self.net>.\n",
    "        \"\"\"\n",
    "        self.net_name = net_name\n",
    "        self.net = build_network(net_name)\n",
    "        self.ae_net = build_network('fmnist_LeNet_rec')\n",
    "\n",
    "    def load_model(self,\n",
    "                   model_path,\n",
    "                   map_location='cuda:1'):\n",
    "        \"\"\"\n",
    "        Load the trained model for the model.\n",
    "        The key here is to initialize <self.c>.\n",
    "        \"\"\"\n",
    "        # Load the general model\n",
    "        model_dict = torch.load(model_path, map_location=map_location)\n",
    "        self.ae_net.load_state_dict(model_dict['net_dict'])\n",
    "        \n",
    "        # Obtain the net dictionary\n",
    "        net_dict = self.net.state_dict()\n",
    "        ae_net_dict = self.ae_net.state_dict()\n",
    "        \n",
    "        # Filter out decoder network keys\n",
    "        ae_net_dict = {k: v for k, v in ae_net_dict.items() if k in net_dict}\n",
    "        \n",
    "        # Overwrite values in the existing state_dict\n",
    "        net_dict.update(ae_net_dict)\n",
    "\n",
    "        # Load the new state_dict\n",
    "        self.net.load_state_dict(net_dict)\n",
    "        \n",
    "\n",
    "    def save_model(self, export_model, save_ae=True):\n",
    "        net_dict = self.net.state_dict()\n",
    "        torch.save({'net_dict': net_dict}, export_model)\n",
    "    \n",
    "    def test(self, train, dataset, device, batch_size, n_jobs_dataloader):\n",
    "        if train:\n",
    "            all_loader, _ = dataset.loaders(batch_size=batch_size,\n",
    "                                            num_workers=n_jobs_dataloader)\n",
    "        else:\n",
    "            all_loader = dataset.loaders(batch_size=batch_size,\n",
    "                                         num_workers=n_jobs_dataloader)\n",
    "        net = self.net.to(device)\n",
    "        criterion = nn.MSELoss(reduction='none')\n",
    "        \n",
    "        n_batches = 0\n",
    "        X_pred_list = []\n",
    "        net.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in all_loader:\n",
    "                X, y, idx = data\n",
    "                X, y, idx = X.to(device), y.to(device), idx.to(device)\n",
    "\n",
    "                X_pred = net(X)\n",
    "                X_pred_list += X_pred\n",
    "        \n",
    "        return np.array(X_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_encoder = RecEncoder()\n",
    "rec_encoder.set_network()\n",
    "rec_encoder.load_model(rec_model_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict_train = {}\n",
    "name_list = ['tshirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "             'sandal', 'shirt', 'sneaker', 'bag', 'boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for you!\n",
      "Almost loaded!\n",
      "Loading dataset for you!\n",
      "Almost loaded!\n",
      "Loading dataset for you!\n",
      "Almost loaded!\n",
      "Loading dataset for you!\n",
      "Almost loaded!\n",
      "Loading dataset for you!\n",
      "Almost loaded!\n",
      "Loading dataset for you!\n",
      "Almost loaded!\n",
      "Loading dataset for you!\n",
      "Almost loaded!\n",
      "Loading dataset for you!\n",
      "Almost loaded!\n",
      "Loading dataset for you!\n",
      "Almost loaded!\n",
      "Loading dataset for you!\n",
      "Almost loaded!\n"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(name_list):\n",
    "    dataset_dict_train[name] = load_dataset(loader_name='fmnist',\n",
    "                                            root=root,\n",
    "                                            label_normal=(i,),\n",
    "                                            ratio_abnormal=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Latent Vector Obtaining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's use the unsupervised model of `pullover` as feature extractor, meaning that the latent space are defined from the neural weights of the unsupervised model of `pullover`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dict_train = {'oc':{}, 'rec':{}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## For One Class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in name_list:\n",
    "    dataset_train = dataset_dict_train[name]\n",
    "    data_train = oc_encoder.test(True, dataset_train, device, 6000, 0)\n",
    "    data_train = np.array([x.cpu().numpy() for x in data_train])\n",
    "    latent_dict_train['oc'][name] = data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## For Rec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in name_list:\n",
    "    dataset_train = dataset_dict_train[name]\n",
    "    data_train = rec_encoder.test(True, dataset_train, device, 6000, 0)\n",
    "    data_train = np.array([x.cpu().numpy() for x in data_train])\n",
    "    latent_dict_train['rec'][name] = data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for KL Divergence by KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cited from https://github.com/nhartland/KL-divergence-estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "def knn_distance(point, sample, k):\n",
    "    \"\"\" \n",
    "    Euclidean distance from `point` to it's `k`-Nearest\n",
    "    Neighbour in `sample` \n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(sample-point, axis=1)\n",
    "    return np.sort(norms)[k]\n",
    "\n",
    "\n",
    "def verify_sample_shapes(s1, s2, k):\n",
    "    # Expects [N, D]\n",
    "    assert(len(s1.shape) == len(s2.shape) == 2)\n",
    "    # Check dimensionality of sample is identical\n",
    "    assert(s1.shape[1] == s2.shape[1])\n",
    "    \n",
    "    \n",
    "def skl_estimator(s1, s2, k=3):\n",
    "    \"\"\" \n",
    "    KL-Divergence estimator using scikit-learn's NearestNeighbours.\n",
    "    Inputs:\n",
    "        s1: (N_1,D) Sample drawn from distribution P\n",
    "        s2: (N_2,D) Sample drawn from distribution Q\n",
    "        k: Number of neighbours considered (default 1)\n",
    "    return: \n",
    "        estimated D(P|Q)\n",
    "    \"\"\"\n",
    "    verify_sample_shapes(s1, s2, k)\n",
    "\n",
    "    n, m = len(s1), len(s2)\n",
    "    d = float(s1.shape[1])\n",
    "    D = np.log(m / (n - 1))\n",
    "\n",
    "    s1_neighbourhood = NearestNeighbors(k + 1, 10).fit(s1)\n",
    "    s2_neighbourhood = NearestNeighbors(k, 10).fit(s2)\n",
    "\n",
    "    for p1 in s1:\n",
    "        s1_distances, indices = s1_neighbourhood.kneighbors([p1], k + 1)\n",
    "        s2_distances, indices = s2_neighbourhood.kneighbors([p1], k)\n",
    "        rho = s1_distances[0][- 1]\n",
    "        nu = s2_distances[0][- 1]\n",
    "        D += (d / n) * np.log(nu / rho)\n",
    "        D += 0\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cited from https://github.com/gregversteeg/NPEET/blob/master/npeet/entropy_estimators.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "from numpy import log\n",
    "from scipy.special import digamma\n",
    "from sklearn.neighbors import BallTree, KDTree\n",
    "\n",
    "# CONTINUOUS ESTIMATORS\n",
    "\n",
    "\n",
    "def entropy(x, k=3, base=2):\n",
    "    \"\"\" The classic K-L k-nearest neighbor continuous entropy estimator\n",
    "        x should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "        if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x = np.asarray(x)\n",
    "    n_elements, n_features = x.shape\n",
    "    x = add_noise(x)\n",
    "    tree = build_tree(x)\n",
    "    nn = query_neighbors(tree, x, k)\n",
    "    const = digamma(n_elements) - digamma(k) + n_features * log(2)\n",
    "    return (const + n_features * np.log(nn).mean()) / log(base)\n",
    "\n",
    "\n",
    "def centropy(x, y, k=3, base=2):\n",
    "    \"\"\" The classic K-L k-nearest neighbor continuous entropy estimator for the\n",
    "        entropy of X conditioned on Y.\n",
    "    \"\"\"\n",
    "    xy = np.c_[x, y]\n",
    "    entropy_union_xy = entropy(xy, k=k, base=base)\n",
    "    entropy_y = entropy(y, k=k, base=base)\n",
    "    return entropy_union_xy - entropy_y\n",
    "\n",
    "\n",
    "def tc(xs, k=3, base=2):\n",
    "    xs_columns = np.expand_dims(xs, axis=0).T\n",
    "    entropy_features = [entropy(col, k=k, base=base) for col in xs_columns]\n",
    "    return np.sum(entropy_features) - entropy(xs, k, base)\n",
    "\n",
    "\n",
    "def ctc(xs, y, k=3, base=2):\n",
    "    xs_columns = np.expand_dims(xs, axis=0).T\n",
    "    centropy_features = [centropy(col, y, k=k, base=base)\n",
    "                         for col in xs_columns]\n",
    "    return np.sum(centropy_features) - centropy(xs, y, k, base)\n",
    "\n",
    "\n",
    "def corex(xs, ys, k=3, base=2):\n",
    "    xs_columns = np.expand_dims(xs, axis=0).T\n",
    "    cmi_features = [mi(col, ys, k=k, base=base) for col in xs_columns]\n",
    "    return np.sum(cmi_features) - mi(xs, ys, k=k, base=base)\n",
    "\n",
    "\n",
    "def mi(x, y, z=None, k=3, base=2, alpha=0):\n",
    "    \"\"\" Mutual information of x and y (conditioned on z if z is not None)\n",
    "        x, y should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "        if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Arrays should have same length\"\n",
    "    assert k <= len(x) - 1, \"Set k smaller than num. samples - 1\"\n",
    "    x, y = np.asarray(x), np.asarray(y)\n",
    "    x, y = x.reshape(x.shape[0], -1), y.reshape(y.shape[0], -1)\n",
    "    x = add_noise(x)\n",
    "    y = add_noise(y)\n",
    "    points = [x, y]\n",
    "    if z is not None:\n",
    "        z = np.asarray(z)\n",
    "        z = z.reshape(z.shape[0], -1)\n",
    "        points.append(z)\n",
    "    points = np.hstack(points)\n",
    "    # Find nearest neighbors in joint space, p=inf means max-norm\n",
    "    tree = build_tree(points)\n",
    "    dvec = query_neighbors(tree, points, k)\n",
    "    if z is None:\n",
    "        a, b, c, d = avgdigamma(x, dvec), avgdigamma(\n",
    "            y, dvec), digamma(k), digamma(len(x))\n",
    "        if alpha > 0:\n",
    "            d += lnc_correction(tree, points, k, alpha)\n",
    "    else:\n",
    "        xz = np.c_[x, z]\n",
    "        yz = np.c_[y, z]\n",
    "        a, b, c, d = avgdigamma(xz, dvec), avgdigamma(\n",
    "            yz, dvec), avgdigamma(z, dvec), digamma(k)\n",
    "    return (-a - b + c + d) / log(base)\n",
    "\n",
    "\n",
    "def cmi(x, y, z, k=3, base=2):\n",
    "    \"\"\" Mutual information of x and y, conditioned on z\n",
    "        Legacy function. Use mi(x, y, z) directly.\n",
    "    \"\"\"\n",
    "    return mi(x, y, z=z, k=k, base=base)\n",
    "\n",
    "\n",
    "def kldiv(x, xp, k=3, base=2):\n",
    "    \"\"\" KL Divergence between p and q for x~p(x), xp~q(x)\n",
    "        x, xp should be a list of vectors, e.g. x = [[1.3], [3.7], [5.1], [2.4]]\n",
    "        if x is a one-dimensional scalar and we have four samples\n",
    "    \"\"\"\n",
    "    assert k < min(len(x), len(xp)), \"Set k smaller than num. samples - 1\"\n",
    "    assert len(x[0]) == len(xp[0]), \"Two distributions must have same dim.\"\n",
    "    x, xp = np.asarray(x), np.asarray(xp)\n",
    "    x, xp = x.reshape(x.shape[0], -1), xp.reshape(xp.shape[0], -1)\n",
    "    d = len(x[0])\n",
    "    n = len(x)\n",
    "    m = len(xp)\n",
    "    const = log(m) - log(n - 1)\n",
    "    tree = build_tree(x)\n",
    "    treep = build_tree(xp)\n",
    "    nn = query_neighbors(tree, x, k)\n",
    "    nnp = query_neighbors(treep, x, k - 1)\n",
    "    return (const + d * (np.log(nnp).mean() - np.log(nn).mean())) / log(base)\n",
    "\n",
    "\n",
    "def lnc_correction(tree, points, k, alpha):\n",
    "    e = 0\n",
    "    n_sample = points.shape[0]\n",
    "    for point in points:\n",
    "        # Find k-nearest neighbors in joint space, p=inf means max norm\n",
    "        knn = tree.query(point[None, :], k=k+1, return_distance=False)[0]\n",
    "        knn_points = points[knn]\n",
    "        # Substract mean of k-nearest neighbor points\n",
    "        knn_points = knn_points - knn_points[0]\n",
    "        # Calculate covariance matrix of k-nearest neighbor points, obtain eigen vectors\n",
    "        covr = knn_points.T @ knn_points / k\n",
    "        _, v = la.eig(covr)\n",
    "        # Calculate PCA-bounding box using eigen vectors\n",
    "        V_rect = np.log(np.abs(knn_points @ v).max(axis=0)).sum()\n",
    "        # Calculate the volume of original box\n",
    "        log_knn_dist = np.log(np.abs(knn_points).max(axis=0)).sum()\n",
    "\n",
    "        # Perform local non-uniformity checking and update correction term\n",
    "        if V_rect < log_knn_dist + np.log(alpha):\n",
    "            e += (log_knn_dist - V_rect) / n_sample\n",
    "    return e\n",
    "\n",
    "\n",
    "# DISCRETE ESTIMATORS\n",
    "def entropyd(sx, base=2):\n",
    "    \"\"\" Discrete entropy estimator\n",
    "        sx is a list of samples\n",
    "    \"\"\"\n",
    "    unique, count = np.unique(sx, return_counts=True, axis=0)\n",
    "    # Convert to float as otherwise integer division results in all 0 for proba.\n",
    "    proba = count.astype(float) / len(sx)\n",
    "    # Avoid 0 division; remove probabilities == 0.0 (removing them does not change the entropy estimate as 0 * log(1/0) = 0.\n",
    "    proba = proba[proba > 0.0]\n",
    "    return np.sum(proba * np.log(1. / proba)) / log(base)\n",
    "\n",
    "\n",
    "def midd(x, y, base=2):\n",
    "    \"\"\" Discrete mutual information estimator\n",
    "        Given a list of samples which can be any hashable object\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Arrays should have same length\"\n",
    "    return entropyd(x, base) - centropyd(x, y, base)\n",
    "\n",
    "\n",
    "def cmidd(x, y, z, base=2):\n",
    "    \"\"\" Discrete mutual information estimator\n",
    "        Given a list of samples which can be any hashable object\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y) == len(z), \"Arrays should have same length\"\n",
    "    xz = np.c_[x, z]\n",
    "    yz = np.c_[y, z]\n",
    "    xyz = np.c_[x, y, z]\n",
    "    return entropyd(xz, base) + entropyd(yz, base) - entropyd(xyz, base) - entropyd(z, base)\n",
    "\n",
    "\n",
    "def centropyd(x, y, base=2):\n",
    "    \"\"\" The classic K-L k-nearest neighbor continuous entropy estimator for the\n",
    "        entropy of X conditioned on Y.\n",
    "    \"\"\"\n",
    "    xy = np.c_[x, y]\n",
    "    return entropyd(xy, base) - entropyd(y, base)\n",
    "\n",
    "\n",
    "def tcd(xs, base=2):\n",
    "    xs_columns = np.expand_dims(xs, axis=0).T\n",
    "    entropy_features = [entropyd(col, base=base) for col in xs_columns]\n",
    "    return np.sum(entropy_features) - entropyd(xs, base)\n",
    "\n",
    "\n",
    "def ctcd(xs, y, base=2):\n",
    "    xs_columns = np.expand_dims(xs, axis=0).T\n",
    "    centropy_features = [centropyd(col, y, base=base) for col in xs_columns]\n",
    "    return np.sum(centropy_features) - centropyd(xs, y, base)\n",
    "\n",
    "\n",
    "def corexd(xs, ys, base=2):\n",
    "    xs_columns = np.expand_dims(xs, axis=0).T\n",
    "    cmi_features = [midd(col, ys, base=base) for col in xs_columns]\n",
    "    return np.sum(cmi_features) - midd(xs, ys, base)\n",
    "\n",
    "\n",
    "# MIXED ESTIMATORS\n",
    "def micd(x, y, k=3, base=2, warning=True):\n",
    "    \"\"\" If x is continuous and y is discrete, compute mutual information\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Arrays should have same length\"\n",
    "    entropy_x = entropy(x, k, base)\n",
    "\n",
    "    y_unique, y_count = np.unique(y, return_counts=True, axis=0)\n",
    "    y_proba = y_count / len(y)\n",
    "\n",
    "    entropy_x_given_y = 0.\n",
    "    for yval, py in zip(y_unique, y_proba):\n",
    "        x_given_y = x[(y == yval).all(axis=1)]\n",
    "        if k <= len(x_given_y) - 1:\n",
    "            entropy_x_given_y += py * entropy(x_given_y, k, base)\n",
    "        else:\n",
    "            if warning:\n",
    "                warnings.warn(\"Warning, after conditioning, on y={yval} insufficient data. \"\n",
    "                              \"Assuming maximal entropy in this case.\".format(yval=yval))\n",
    "            entropy_x_given_y += py * entropy_x\n",
    "    return abs(entropy_x - entropy_x_given_y)  # units already applied\n",
    "\n",
    "\n",
    "def midc(x, y, k=3, base=2, warning=True):\n",
    "    return micd(y, x, k, base, warning)\n",
    "\n",
    "\n",
    "def centropycd(x, y, k=3, base=2, warning=True):\n",
    "    return entropy(x, base) - micd(x, y, k, base, warning)\n",
    "\n",
    "\n",
    "def centropydc(x, y, k=3, base=2, warning=True):\n",
    "    return centropycd(y, x, k=k, base=base, warning=warning)\n",
    "\n",
    "\n",
    "def ctcdc(xs, y, k=3, base=2, warning=True):\n",
    "    xs_columns = np.expand_dims(xs, axis=0).T\n",
    "    centropy_features = [centropydc(\n",
    "        col, y, k=k, base=base, warning=warning) for col in xs_columns]\n",
    "    return np.sum(centropy_features) - centropydc(xs, y, k, base, warning)\n",
    "\n",
    "\n",
    "def ctccd(xs, y, k=3, base=2, warning=True):\n",
    "    return ctcdc(y, xs, k=k, base=base, warning=warning)\n",
    "\n",
    "\n",
    "def corexcd(xs, ys, k=3, base=2, warning=True):\n",
    "    return corexdc(ys, xs, k=k, base=base, warning=warning)\n",
    "\n",
    "\n",
    "def corexdc(xs, ys, k=3, base=2, warning=True):\n",
    "    return tcd(xs, base) - ctcdc(xs, ys, k, base, warning)\n",
    "\n",
    "\n",
    "# UTILITY FUNCTIONS\n",
    "\n",
    "def add_noise(x, intens=1e-10):\n",
    "    # small noise to break degeneracy, see doc.\n",
    "    return x + intens * np.random.random_sample(x.shape)\n",
    "\n",
    "\n",
    "def query_neighbors(tree, x, k):\n",
    "    return tree.query(x, k=k + 1)[0][:, k]\n",
    "\n",
    "\n",
    "def count_neighbors(tree, x, r):\n",
    "    return tree.query_radius(x, r, count_only=True)\n",
    "\n",
    "\n",
    "def avgdigamma(points, dvec):\n",
    "    # This part finds number of neighbors in some radius in the marginal space\n",
    "    # returns expectation value of <psi(nx)>\n",
    "    tree = build_tree(points)\n",
    "    dvec = dvec - 1e-15\n",
    "    num_points = count_neighbors(tree, points, dvec)\n",
    "    return np.mean(digamma(num_points))\n",
    "\n",
    "\n",
    "def build_tree(points):\n",
    "    if points.shape[1] >= 20:\n",
    "        return BallTree(points, metric='chebyshev')\n",
    "    return KDTree(points, metric='chebyshev')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cited from universal_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from universal_divergence import estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Joint KL Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## For One Class Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-sysmetric KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -2.354828129099568\n",
      "0 1 -1.8498137466923816\n",
      "0 2 -2.274063666910703\n",
      "0 3 -2.242290256144239\n",
      "0 4 -2.1882307533006604\n",
      "0 5 -1.7123093569054735\n",
      "0 6 -2.894960748697684\n",
      "0 7 -1.6864171224097406\n",
      "0 8 -1.8567664866270666\n",
      "0 9 -1.6933993284098678\n",
      "1 0 -2.6618202267996636\n",
      "1 1 -3.115679273424921\n",
      "1 2 -2.5970491260162007\n",
      "1 3 -2.805409003225345\n",
      "1 4 -2.590596954465307\n",
      "1 5 -2.4481038387245455\n",
      "1 6 -2.6492309494601654\n",
      "1 7 -2.4366167366676796\n",
      "1 8 -2.452341977601175\n",
      "1 9 -2.4314755296429027\n",
      "2 0 -0.9095226750060134\n",
      "2 1 -0.7739677100067145\n",
      "2 2 -1.4235522350949072\n",
      "2 3 -0.8656402623814345\n",
      "2 4 -1.4454330555553514\n",
      "2 5 -0.7463445561455211\n",
      "2 6 -1.4670334321028835\n",
      "2 7 -0.7440985609887868\n",
      "2 8 -0.7865783539429099\n",
      "2 9 -0.7466445176547827\n",
      "3 0 -2.5039259553289424\n",
      "3 1 -2.1027065501582327\n",
      "3 2 -2.4008056479403845\n",
      "3 3 -2.5023997645982634\n",
      "3 4 -2.5749480125630986\n",
      "3 5 -1.8560076321364247\n",
      "3 6 -2.5176900646839804\n",
      "3 7 -1.8305902517532102\n",
      "3 8 -1.8911254106015596\n",
      "3 9 -1.822694665820784\n",
      "4 0 -1.3612307402300714\n",
      "4 1 -1.2283004122765049\n",
      "4 2 -2.632999577389034\n",
      "4 3 -1.5676804442144754\n",
      "4 4 -1.8128519002163577\n",
      "4 5 -1.135651682704179\n",
      "4 6 -2.286579780733342\n",
      "4 7 -1.1263247727818002\n",
      "4 8 -1.2482237368844287\n",
      "4 9 -1.1410066390510554\n",
      "5 0 -1.5166428688478373\n",
      "5 1 -1.4655747350135597\n",
      "5 2 -1.5118482832766322\n",
      "5 3 -1.5224165673989656\n",
      "5 4 -1.5014796064100429\n",
      "5 5 -2.1284693464648137\n",
      "5 6 -1.524508972044998\n",
      "5 7 -2.3733816927812073\n",
      "5 8 -1.567588860510335\n",
      "5 9 -2.079678802302058\n",
      "6 0 -2.607548787405163\n",
      "6 1 -1.2952565463903618\n",
      "6 2 -2.4392247324623684\n",
      "6 3 -1.6469777565722308\n",
      "6 4 -2.179530261335593\n",
      "6 5 -1.1638581037267097\n",
      "6 6 -1.8134886003394624\n",
      "6 7 -1.143396102713396\n",
      "6 8 -1.239481790101303\n",
      "6 9 -1.1490466184758779\n",
      "7 0 -1.5537829096350269\n",
      "7 1 -1.5488369743248933\n",
      "7 2 -1.560746619121946\n",
      "7 3 -1.5507483730344767\n",
      "7 4 -1.554118504153789\n",
      "7 5 -1.7814590183796397\n",
      "7 6 -1.5604963919299628\n",
      "7 7 -2.235845300391535\n",
      "7 8 -1.5676636863993572\n",
      "7 9 -2.1241121337971713\n",
      "8 0 -2.0982183408210635\n",
      "8 1 -1.8807413831449\n",
      "8 2 -2.310766040126008\n",
      "8 3 -2.0524430683225203\n",
      "8 4 -2.2354958988940883\n",
      "8 5 -1.978223958113036\n",
      "8 6 -2.2926036785895048\n",
      "8 7 -1.9569513682465336\n",
      "8 8 -2.4482493984681772\n",
      "8 9 -1.8481170412466184\n",
      "9 0 -2.0922324582513667\n",
      "9 1 -2.0726611545083578\n",
      "9 2 -2.0989013021648497\n",
      "9 3 -2.0944922864333955\n",
      "9 4 -2.092397777872989\n",
      "9 5 -2.2114736924883958\n",
      "9 6 -2.0985722223962364\n",
      "9 7 -2.5667476534736475\n",
      "9 8 -2.0972451547563518\n",
      "9 9 -2.74296847192765\n"
     ]
    }
   ],
   "source": [
    "oc_gram = np.zeros((10, 10))\n",
    "\n",
    "for i, x_i in enumerate(latent_dict_train['oc']):\n",
    "    for j, x_j in enumerate(latent_dict_train['oc']):\n",
    "        left = latent_dict_train['oc'][x_i]\n",
    "        if i == j:\n",
    "            right = left\n",
    "        else:\n",
    "            right = np.r_[latent_dict_train['oc'][x_i], latent_dict_train['oc'][x_j]]\n",
    "        oc_gram[i][j] = skl_estimator(left, right)\n",
    "        print(i, j, oc_gram[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['misc/oc_gram.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(oc_gram, 'misc/oc_gram.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -2.354828129099568\n",
      "0 1 8.372350223130455\n",
      "0 2 0.632486993778488\n",
      "0 3 2.8342967265786028\n",
      "0 4 0.7207779485948227\n",
      "0 5 3.181389318121882\n",
      "0 6 -0.10480073219150121\n",
      "0 7 10.520922675282398\n",
      "0 8 5.23345918853433\n",
      "0 9 12.780899299240435\n",
      "1 0 3.278694874573044\n",
      "1 1 -3.115679273424921\n",
      "1 2 2.2272192968072693\n",
      "1 3 1.7735896929705406\n",
      "1 4 1.8507408675835626\n",
      "1 5 3.689556755682486\n",
      "1 6 1.9053755238472616\n",
      "1 7 11.054650957051862\n",
      "1 8 6.429171433353918\n",
      "1 9 16.904121054736237\n",
      "2 0 3.0740788336782523\n",
      "2 1 9.907598697391228\n",
      "2 2 -1.4235522350949072\n",
      "2 3 4.83690802965049\n",
      "2 4 -0.2028645714832764\n",
      "2 5 3.716571587155955\n",
      "2 6 0.1046149715085074\n",
      "2 7 11.127540019082039\n",
      "2 8 6.143370770701506\n",
      "2 9 17.297589896438282\n",
      "3 0 2.2007246800976885\n",
      "3 1 6.863260537629159\n",
      "3 2 1.3063262944065508\n",
      "3 3 -2.5023997645982634\n",
      "3 4 0.7880629328969462\n",
      "3 5 3.206709653428155\n",
      "3 6 0.8678840181378191\n",
      "3 7 10.499469752299891\n",
      "3 8 6.138952229344589\n",
      "3 9 15.006995068905713\n",
      "4 0 2.9758632320241496\n",
      "4 1 9.212123149389848\n",
      "4 2 -0.5351465797876966\n",
      "4 3 3.783267310914543\n",
      "4 4 -1.8128519002163577\n",
      "4 5 3.600662485709961\n",
      "4 6 -0.10194294773171197\n",
      "4 7 10.811254794841693\n",
      "4 8 5.301648818609119\n",
      "4 9 16.34025854724972\n",
      "5 0 5.085482776963895\n",
      "5 1 11.27990474853724\n"
     ]
    }
   ],
   "source": [
    "oc_gram_sys = np.zeros((10, 10))\n",
    "\n",
    "for i, x_i in enumerate(latent_dict_train['oc']):\n",
    "    for j, x_j in enumerate(latent_dict_train['oc']):\n",
    "        left = latent_dict_train['oc'][x_i]\n",
    "        if i == j:\n",
    "            right = left \n",
    "        else:\n",
    "            right = np.r_[latent_dict_train['oc'][x_i], latent_dict_train['oc'][x_j]]\n",
    "            \n",
    "        oc_gram_sys[i][j] = 0.5 * (skl_estimator(left, right) + skl_estimator(right, left))\n",
    "        \n",
    "        print(i, j, oc_gram_sys[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(oc_gram_sys, 'misc/oc_gram_sys.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For Rec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_gram = np.zeros((10, 10))\n",
    "\n",
    "for i, x_i in enumerate(latent_dict_train['rec']):\n",
    "    for j, x_j in enumerate(latent_dict_train['rec']):\n",
    "        left = latent_dict_train['rec'][x_i]\n",
    "        if i == j:\n",
    "            right = left\n",
    "        else:\n",
    "            right = np.r_[latent_dict_train['rec'][x_i], latent_dict_train['rec'][x_j]]\n",
    "        rec_gram[i][j] = skl_estimator(left, right)\n",
    "        print(i, j, rec_gram[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rec_gram, 'misc/rec_gram.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tshirt',\n",
       " 'trouser',\n",
       " 'pullover',\n",
       " 'dress',\n",
       " 'coat',\n",
       " 'sandal',\n",
       " 'shirt',\n",
       " 'sneaker',\n",
       " 'bag',\n",
       " 'boot']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -2.250281069595419\n",
      "0 1 15.824935617870292\n",
      "0 2 6.131335384522765\n",
      "0 3 7.865118500694239\n",
      "0 4 6.801428913329501\n",
      "0 5 15.369509977835822\n",
      "0 6 2.214840167942079\n",
      "0 7 20.705115201901734\n",
      "0 8 11.321564940618764\n",
      "0 9 20.575184344212254\n",
      "1 0 15.824935617870292\n",
      "1 1 -2.4486417116658665\n",
      "1 2 13.631926647486072\n",
      "1 3 10.542556604341224\n",
      "1 4 12.410020612213536\n",
      "1 5 23.343266090583885\n",
      "1 6 12.66060688836089\n",
      "1 7 28.96559045677866\n",
      "1 8 18.514331869247336\n",
      "1 9 28.755787119534908\n",
      "2 0 6.131335384522765\n",
      "2 1 13.631926647486072\n",
      "2 2 8.333680574837545e-05\n",
      "2 3 7.041352319240344\n",
      "2 4 2.115061431801671\n",
      "2 5 10.916015391815506\n",
      "2 6 2.3789507767714513\n",
      "2 7 16.4161500474098\n",
      "2 8 6.818326126430715\n",
      "2 9 16.50732695055503\n",
      "3 0 7.865118500694239\n",
      "3 1 10.542556604341224\n",
      "3 2 7.041352319240344\n",
      "3 3 -2.23631878381988\n",
      "3 4 5.802529730129007\n",
      "3 5 16.774812288207997\n",
      "3 6 5.676743697118971\n",
      "3 7 22.43947845798847\n",
      "3 8 11.932246505128951\n",
      "3 9 22.035742919290225\n",
      "4 0 6.801428913329501\n",
      "4 1 12.410020612213536\n",
      "4 2 2.115061431801671\n",
      "4 3 5.802529730129007\n",
      "4 4 -2.150371681549348\n",
      "4 5 11.767593275763936\n",
      "4 6 2.3745828569445933\n",
      "4 7 17.491159073020945\n",
      "4 8 7.379931322421027\n",
      "4 9 17.09871225751686\n",
      "5 0 15.369509977835822\n",
      "5 1 23.343266090583885\n",
      "5 2 10.916015391815506\n",
      "5 3 16.774812288207997\n",
      "5 4 11.767593275763936\n",
      "5 5 -2.22693691894272\n"
     ]
    }
   ],
   "source": [
    "rec_gram_sys_ = np.zeros((10, 10))\n",
    "\n",
    "for i, x_i in enumerate(latent_dict_train['rec']):\n",
    "    for j, x_j in enumerate(latent_dict_train['rec']):\n",
    "        left = np.r_[latent_dict_train['rec']['pullover'], latent_dict_train['rec'][x_i]]\n",
    "        right = np.r_[latent_dict_train['rec']['pullover'], latent_dict_train['rec'][x_j]]\n",
    "        rec_gram_sys_[i][j] = 0.5 * (skl_estimator(left, right) + skl_estimator(right, left))\n",
    "        \n",
    "        print(i, j, rec_gram_sys_[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0 14.357915409695117\n",
      "8 1 21.10255600807521\n",
      "8 2 8.172230218238127\n",
      "8 3 14.341366345895981\n",
      "8 4 8.74155004088028\n",
      "8 5 13.306805792042411\n"
     ]
    }
   ],
   "source": [
    "rec_gram_sys_ = np.zeros((10, 10))\n",
    "\n",
    "for i, x_i in enumerate(latent_dict_train['rec']):\n",
    "    if i < 8:\n",
    "        continue\n",
    "    for j, x_j in enumerate(latent_dict_train['rec']):\n",
    "        left = np.r_[latent_dict_train['rec']['pullover'], latent_dict_train['rec'][x_i]]\n",
    "        right = np.r_[latent_dict_train['rec']['pullover'], latent_dict_train['rec'][x_j]]\n",
    "        rec_gram_sys_[i][j] = 0.5 * (skl_estimator(left, right) + skl_estimator(right, left))\n",
    "        \n",
    "        print(i, j, rec_gram_sys_[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 -2.374685204233279\n",
      "0 1 9.388758905429942\n",
      "0 2 4.3290357052324815\n",
      "0 3 3.2845118312785173\n",
      "0 4 5.1263364119147266\n",
      "0 5 7.469907312417979\n",
      "0 6 2.053953081168619\n",
      "0 7 11.668403525707204\n",
      "0 8 5.949233584698067\n",
      "0 9 13.670346892646844\n",
      "1 0 7.479061788144726\n",
      "1 1 -2.765432814088853\n",
      "1 2 5.703915914330305\n",
      "1 3 2.8157584627452206\n",
      "1 4 6.096560815277588\n",
      "1 5 9.478108448500471\n",
      "1 6 5.735912597748136\n",
      "1 7 14.956690648832325\n",
      "1 8 9.719402270143139\n",
      "1 9 16.033443470553024\n",
      "2 0 5.270507654839799\n",
      "2 1 11.671844714250408\n",
      "2 2 -2.3990006264736086\n",
      "2 3 5.766454930991621\n",
      "2 4 0.30726571764174415\n",
      "2 5 6.771600542884427\n",
      "2 6 0.7740232262350699\n",
      "2 7 11.295032210743338\n",
      "2 8 5.120180500255494\n",
      "2 9 13.086691814345226\n",
      "3 0 4.620254172817776\n",
      "3 1 7.129297708793364\n",
      "3 2 5.364664581746643\n",
      "3 3 -2.318751138820908\n",
      "3 4 4.471196458475208\n",
      "3 5 8.590257434434607\n",
      "3 6 3.8091844822986873\n",
      "3 7 13.215546444533558\n",
      "3 8 7.861650120004175\n",
      "3 9 15.245930860936328\n",
      "4 0 6.248791034143045\n",
      "4 1 9.652452675474443\n",
      "4 2 0.34674793375192925\n",
      "4 3 4.336442671401848\n",
      "4 4 -2.190952708353413\n",
      "4 5 7.36827713612698\n",
      "4 6 1.192791789264155\n",
      "4 7 12.447329223220047\n",
      "4 8 5.694319840940256\n",
      "4 9 13.351253455980796\n",
      "5 0 13.168803398268176\n",
      "5 1 18.480998078966532\n",
      "5 2 12.22978799853841\n",
      "5 3 13.351211649815191\n",
      "5 4 13.33305947929928\n",
      "5 5 -2.100646229224489\n",
      "5 6 11.157207705632095\n",
      "5 7 2.504761656561203\n",
      "5 8 7.542311678949255\n",
      "5 9 5.730468579688524\n",
      "6 0 0.46617817295816644\n",
      "6 1 9.957063786833851\n",
      "6 2 0.361943102002944\n",
      "6 3 3.5261970033373857\n",
      "6 4 0.5939758049947765\n",
      "6 5 6.538404286714799\n",
      "6 6 -2.3009491505583073\n",
      "6 7 10.99263823456501\n",
      "6 8 4.735953208364569\n",
      "6 9 12.799880239169939\n",
      "7 0 15.60462918005091\n",
      "7 1 20.456985111889885\n",
      "7 2 14.132650775544052\n",
      "7 3 15.603361604239934\n",
      "7 4 14.913007564601914\n",
      "7 5 1.8147544202135633\n",
      "7 6 12.934197846127882\n",
      "7 7 -2.018341594508178\n",
      "7 8 7.6628136854413285\n",
      "7 9 4.119397340777776\n",
      "8 0 8.492127970497686\n",
      "8 1 12.94461150371583\n",
      "8 2 6.108887046832293\n",
      "8 3 7.300572339559773\n",
      "8 4 6.514496868640608\n",
      "8 5 4.883912746776558\n",
      "8 6 5.7842239052162405\n",
      "8 7 7.188999309656952\n",
      "8 8 -2.1517278410501777\n",
      "8 9 10.057121907635972\n",
      "9 0 13.77270231953836\n",
      "9 1 18.798268109201608\n",
      "9 2 12.558876620860799\n",
      "9 3 13.508427571786978\n",
      "9 4 13.162765114009883\n",
      "9 5 3.3434242444657323\n",
      "9 6 11.741117723884821\n",
      "9 7 4.5441503380896044\n",
      "9 8 7.6707217717537555\n",
      "9 9 -2.179383528165716\n"
     ]
    }
   ],
   "source": [
    "rec_gram_sys_ = np.zeros((10, 10))\n",
    "\n",
    "for i, x_i in enumerate(latent_dict_train['rec']):\n",
    "    for j, x_j in enumerate(latent_dict_train['rec']):\n",
    "        left = latent_dict_train['rec'][x_i]\n",
    "        if i == j:\n",
    "            right = left \n",
    "        else:\n",
    "            right = np.r_[latent_dict_train['rec'][x_i], latent_dict_train['rec'][x_j]]\n",
    "            \n",
    "        rec_gram_sys[i][j] = 0.5 * (skl_estimator(left, right) + skl_estimator(right, left))\n",
    "        \n",
    "        print(i, j, rec_gram_sys[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['misc/rec_gram_sys.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rec_gram_sys, 'misc/rec_gram_sys.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load for extra data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pullover_train = latent_dict_train['oc']['pullover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pullover_test = load_dataset(loader_name='fmnist_eval',\n",
    "                                     root=root,\n",
    "                                     label_eval=(2,),\n",
    "                                     test_eval=True)\n",
    "pullover_test = oc_encoder.test(False, dataset_pullover_test, device, 1000, 0)\n",
    "pullover_test = np.array([x.cpu().numpy() for x in pullover_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_coat_test = load_dataset(loader_name='fmnist_eval',\n",
    "                                     root=root,\n",
    "                                     label_eval=(4,),\n",
    "                                     test_eval=True)\n",
    "coat_test = oc_encoder.test(False, dataset_coat_test, device, 1000, 0)\n",
    "coat_test = np.array([x.cpu().numpy() for x in coat_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sneaker_test = load_dataset(loader_name='fmnist_eval',\n",
    "                                     root=root,\n",
    "                                     label_eval=(7,),\n",
    "                                     test_eval=True)\n",
    "sneaker_test = oc_encoder.test(False, dataset_sneaker_test, device, 1000, 0)\n",
    "sneaker_test = np.array([x.cpu().numpy() for x in sneaker_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pullover_coat_train = np.r_[latent_dict_train['oc']['pullover'], latent_dict_train['oc']['coat']]\n",
    "pullover_sneaker_train = np.r_[latent_dict_train['oc']['pullover'], latent_dict_train['oc']['sneaker']]\n",
    "\n",
    "pullover_coat_test_ = np.r_[pullover_test, coat_test]\n",
    "pullover_sneaker_test_ = np.r_[pullover_test, sneaker_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Pullover & Coat**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Joint Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pullover_coat = {}\n",
    "dict_pullover_coat['coat'] = skl_estimator(pullover_coat_train, pullover_coat_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tshirt 6.678524225035061\n",
      "trouser 7.185524170868892\n",
      "dress 6.070237151115829\n",
      "sandal 7.661263468168078\n",
      "shirt 3.8714684233428267\n",
      "sneaker 7.699502890825909\n",
      "bag 7.41631880409517\n",
      "boot 7.716410785422828\n"
     ]
    }
   ],
   "source": [
    "for x in name_list:\n",
    "    if x in ['coat', 'pullover']:\n",
    "        continue\n",
    "    \n",
    "    joint = np.r_[pullover_test, latent_dict_all['oc'][x]]\n",
    "    kl_ = skl_estimator(pullover_coat_train, joint)\n",
    "    dict_pullover_coat[x] = kl_\n",
    "    print(x, kl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coat': 7.661263468168078,\n",
       " 'tshirt': 6.678524225035061,\n",
       " 'trouser': 7.185524170868892,\n",
       " 'dress': 6.070237151115829,\n",
       " 'sandal': 7.661263468168078,\n",
       " 'shirt': 3.8714684233428267,\n",
       " 'sneaker': 7.699502890825909,\n",
       " 'bag': 7.41631880409517,\n",
       " 'boot': 7.716410785422828}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pullover_coat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Marginal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pullover_coat_ = {}\n",
    "dict_pullover_coat_['training divergence'] = skl_estimator(pullover_train, latent_dict_train['oc']['coat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tshirt 11.072399005169489\n",
      "trouser 18.745953300786717\n",
      "dress 13.76077460303965\n",
      "coat 7.160040157098573\n",
      "sandal 21.11867218781776\n",
      "shirt 4.209448972482385\n",
      "sneaker 23.227279890195163\n",
      "bag 21.15262816948465\n",
      "boot 27.834980624381743\n"
     ]
    }
   ],
   "source": [
    "for x in name_list:\n",
    "    if x in ['pullover']:\n",
    "        continue\n",
    "        \n",
    "    if x in ['coat']:\n",
    "        marginal = coat_test\n",
    "    else:\n",
    "        marginal = latent_dict_all['oc'][x]\n",
    "        \n",
    "    kl_ = skl_estimator(pullover_train, marginal)\n",
    "    dict_pullover_coat_[x] = kl_\n",
    "    print(x, kl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training divergence': 4.568845462776275,\n",
       " 'tshirt': 11.072399005169489,\n",
       " 'trouser': 18.745953300786717,\n",
       " 'dress': 13.76077460303965,\n",
       " 'coat': 7.160040157098573,\n",
       " 'sandal': 21.11867218781776,\n",
       " 'shirt': 4.209448972482385,\n",
       " 'sneaker': 23.227279890195163,\n",
       " 'bag': 21.15262816948465,\n",
       " 'boot': 27.834980624381743}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pullover_coat_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Pullover & Sneaker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Joint Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pullover_sneaker = {}\n",
    "dict_pullover_sneaker['sneaker'] = skl_estimator(pullover_sneaker_train, pullover_sneaker_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tshirt 27.749288133239418\n",
      "trouser 28.80863523884461\n",
      "dress 27.979325455792644\n",
      "coat 26.085610547476527\n",
      "sandal 12.502880735513374\n",
      "shirt 25.872357051664995\n",
      "bag 21.306602137031792\n",
      "boot 14.474244197800164\n"
     ]
    }
   ],
   "source": [
    "for x in name_list:\n",
    "    if x in ['sneaker', 'pullover']:\n",
    "        continue\n",
    "    \n",
    "    joint = np.r_[pullover_test, latent_dict_all['oc'][x]]\n",
    "    kl_ = skl_estimator(pullover_sneaker_train, joint)\n",
    "    dict_pullover_sneaker[x] = kl_\n",
    "    print(x, kl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sneaker': 4.945477682473212,\n",
       " 'tshirt': 27.749288133239418,\n",
       " 'trouser': 28.80863523884461,\n",
       " 'dress': 27.979325455792644,\n",
       " 'coat': 26.085610547476527,\n",
       " 'sandal': 12.502880735513374,\n",
       " 'shirt': 25.872357051664995,\n",
       " 'bag': 21.306602137031792,\n",
       " 'boot': 14.474244197800164}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pullover_sneaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Marginal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pullover_sneaker_ = {}\n",
    "dict_pullover_sneaker_['training divergence'] = skl_estimator(pullover_train, latent_dict_train['oc']['sneaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tshirt 11.072399005169489\n",
      "trouser 18.745953300786717\n",
      "dress 13.76077460303965\n",
      "coat 4.339410330539567\n",
      "sandal 21.11867218781776\n",
      "shirt 4.209448972482385\n",
      "sneaker 24.12124078252928\n",
      "bag 21.15262816948465\n",
      "boot 27.834980624381743\n"
     ]
    }
   ],
   "source": [
    "for x in name_list:\n",
    "    if x in ['pullover']:\n",
    "        continue\n",
    "        \n",
    "    if x in ['sneaker']:\n",
    "        marginal = sneaker_test\n",
    "    else:\n",
    "        marginal = latent_dict_all['oc'][x]\n",
    "        \n",
    "    kl_ = skl_estimator(pullover_train, marginal)\n",
    "    dict_pullover_sneaker_[x] = kl_\n",
    "    print(x, kl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training divergence': 23.674082066947456,\n",
       " 'tshirt': 11.072399005169489,\n",
       " 'trouser': 18.745953300786717,\n",
       " 'dress': 13.76077460303965,\n",
       " 'coat': 4.339410330539567,\n",
       " 'sandal': 21.11867218781776,\n",
       " 'shirt': 4.209448972482385,\n",
       " 'sneaker': 24.12124078252928,\n",
       " 'bag': 21.15262816948465,\n",
       " 'boot': 27.834980624381743}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pullover_sneaker_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## For Reconstruction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "pullover_train = latent_dict_train['rec']['pullover']\n",
    "\n",
    "dataset_pullover_test = load_dataset(loader_name='fmnist_eval',\n",
    "                                     root=root,\n",
    "                                     label_eval=(2,),\n",
    "                                     test_eval=True)\n",
    "pullover_test = rec_encoder.test(False, dataset_pullover_test, device, 1000, 0)\n",
    "pullover_test = np.array([x.cpu().numpy() for x in pullover_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_coat_test = load_dataset(loader_name='fmnist_eval',\n",
    "                                     root=root,\n",
    "                                     label_eval=(4,),\n",
    "                                     test_eval=True)\n",
    "coat_test = rec_encoder.test(False, dataset_coat_test, device, 1000, 0)\n",
    "coat_test = np.array([x.cpu().numpy() for x in coat_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sneaker_test = load_dataset(loader_name='fmnist_eval',\n",
    "                                     root=root,\n",
    "                                     label_eval=(7,),\n",
    "                                     test_eval=True)\n",
    "sneaker_test = rec_encoder.test(False, dataset_sneaker_test, device, 1000, 0)\n",
    "sneaker_test = np.array([x.cpu().numpy() for x in sneaker_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pullover_coat_train = np.r_[latent_dict_train['rec']['pullover'], latent_dict_train['rec']['coat']]\n",
    "pullover_sneaker_train = np.r_[latent_dict_train['rec']['pullover'], latent_dict_train['rec']['sneaker']]\n",
    "\n",
    "pullover_coat_test_ = np.r_[pullover_test, coat_test]\n",
    "pullover_sneaker_test_ = np.r_[pullover_test, sneaker_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Pullover & Coat**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Joint Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pullover_coat = {}\n",
    "dict_pullover_coat['coat'] = skl_estimator(pullover_coat_train, pullover_coat_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tshirt 11.120735874828831\n",
      "trouser 11.637051307975428\n",
      "dress 9.935287005951649\n",
      "sandal 12.333502323614523\n",
      "shirt 6.703082442223101\n",
      "sneaker 12.347926201399233\n",
      "bag 11.785093318871137\n",
      "boot 12.351785909246662\n"
     ]
    }
   ],
   "source": [
    "for x in name_list:\n",
    "    if x in ['coat', 'pullover']:\n",
    "        continue\n",
    "    \n",
    "    joint = np.r_[pullover_test, latent_dict_all['rec'][x]]\n",
    "    kl_ = skl_estimator(pullover_coat_train, joint)\n",
    "    dict_pullover_coat[x] = kl_\n",
    "    print(x, kl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coat': 5.7802951521432515,\n",
       " 'tshirt': 11.120735874828831,\n",
       " 'trouser': 11.637051307975428,\n",
       " 'dress': 9.935287005951649,\n",
       " 'sandal': 12.333502323614523,\n",
       " 'shirt': 6.703082442223101,\n",
       " 'sneaker': 12.347926201399233,\n",
       " 'bag': 11.785093318871137,\n",
       " 'boot': 12.351785909246662}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pullover_coat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Marginal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pullover_coat_ = {}\n",
    "dict_pullover_coat_['training divergence'] = skl_estimator(pullover_train, latent_dict_train['rec']['coat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tshirt 22.419006960163852\n",
      "trouser 29.222279733304465\n",
      "dress 25.72757446330517\n",
      "coat 13.3593069721264\n",
      "sandal 42.72670602945978\n",
      "shirt 8.374302806296916\n",
      "sneaker 51.56025831856643\n",
      "bag 27.437054142348256\n",
      "boot 42.617041903532666\n"
     ]
    }
   ],
   "source": [
    "for x in name_list:\n",
    "    if x in ['pullover']:\n",
    "        continue\n",
    "        \n",
    "    if x in ['coat']:\n",
    "        marginal = coat_test\n",
    "    else:\n",
    "        marginal = latent_dict_all['rec'][x]\n",
    "        \n",
    "    kl_ = skl_estimator(pullover_train, marginal)\n",
    "    dict_pullover_coat_[x] = kl_\n",
    "    print(x, kl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training divergence': 8.94335837191094,\n",
       " 'tshirt': 22.419006960163852,\n",
       " 'trouser': 29.222279733304465,\n",
       " 'dress': 25.72757446330517,\n",
       " 'coat': 13.3593069721264,\n",
       " 'sandal': 42.72670602945978,\n",
       " 'shirt': 8.374302806296916,\n",
       " 'sneaker': 51.56025831856643,\n",
       " 'bag': 27.437054142348256,\n",
       " 'boot': 42.617041903532666}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pullover_coat_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Pullover & Sneaker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Joint Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pullover_sneaker = {}\n",
    "dict_pullover_sneaker['sneaker'] = skl_estimator(pullover_sneaker_train, pullover_sneaker_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tshirt 27.240742390932915\n",
      "trouser 32.03923934051166\n",
      "dress 30.509918074422842\n",
      "coat 28.90765698237408\n",
      "sandal 11.615368704008327\n",
      "shirt 25.05988784523028\n",
      "bag 18.78444655946627\n",
      "boot 13.841945709052613\n"
     ]
    }
   ],
   "source": [
    "for x in name_list:\n",
    "    if x in ['sneaker', 'pullover']:\n",
    "        continue\n",
    "    \n",
    "    joint = np.r_[pullover_test, latent_dict_all['rec'][x]]\n",
    "    kl_ = skl_estimator(pullover_sneaker_train, joint)\n",
    "    dict_pullover_sneaker[x] = kl_\n",
    "    print(x, kl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sneaker': 5.553674691659444,\n",
       " 'tshirt': 27.240742390932915,\n",
       " 'trouser': 32.03923934051166,\n",
       " 'dress': 30.509918074422842,\n",
       " 'coat': 28.90765698237408,\n",
       " 'sandal': 11.615368704008327,\n",
       " 'shirt': 25.05988784523028,\n",
       " 'bag': 18.78444655946627,\n",
       " 'boot': 13.841945709052613}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pullover_sneaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Marginal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pullover_sneaker_ = {}\n",
    "dict_pullover_sneaker_['training divergence'] = skl_estimator(pullover_train, latent_dict_train['rec']['sneaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tshirt 22.419006960163852\n",
      "trouser 29.222279733304465\n",
      "dress 25.72757446330517\n",
      "coat 8.561594577078122\n",
      "sandal 42.72670602945978\n",
      "shirt 8.374302806296916\n",
      "sneaker 58.13651305406425\n",
      "bag 27.437054142348256\n",
      "boot 42.617041903532666\n"
     ]
    }
   ],
   "source": [
    "for x in name_list:\n",
    "    if x in ['pullover']:\n",
    "        continue\n",
    "        \n",
    "    if x in ['sneaker']:\n",
    "        marginal = sneaker_test\n",
    "    else:\n",
    "        marginal = latent_dict_all['rec'][x]\n",
    "        \n",
    "    kl_ = skl_estimator(pullover_train, marginal)\n",
    "    dict_pullover_sneaker_[x] = kl_\n",
    "    print(x, kl_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training divergence': 51.440300511933614,\n",
       " 'tshirt': 22.419006960163852,\n",
       " 'trouser': 29.222279733304465,\n",
       " 'dress': 25.72757446330517,\n",
       " 'coat': 8.561594577078122,\n",
       " 'sandal': 42.72670602945978,\n",
       " 'shirt': 8.374302806296916,\n",
       " 'sneaker': 58.13651305406425,\n",
       " 'bag': 27.437054142348256,\n",
       " 'boot': 42.617041903532666}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pullover_sneaker_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
